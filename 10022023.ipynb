{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf1476c",
   "metadata": {},
   "source": [
    "ANSWER-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd66675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# User input for the product to search for\n",
    "search_query = input(\"Enter the product you want to search for: \")\n",
    "\n",
    "# Initialize Selenium WebDriver (you need to provide the path to your Chrome WebDriver)\n",
    "chrome_driver_path = 'path/to/your/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Open Amazon.in\n",
    "driver.get(\"https://www.amazon.in\")\n",
    "\n",
    "# Find the search input field and enter the search query\n",
    "search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_box.clear()\n",
    "search_box.send_keys(search_query)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the results page to load (you may need to adjust the sleep duration)\n",
    "time.sleep(3)\n",
    "\n",
    "# Parse the search results page with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find and print product titles and prices\n",
    "product_results = soup.find_all('div', class_='s-result-item')\n",
    "for product in product_results:\n",
    "    title = product.find('span', class_='a-text-normal').text.strip()\n",
    "    price = product.find('span', class_='a-price-whole').text.strip()\n",
    "    print(f\"Title: {title}\\nPrice: {price}\\n\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b7a1d",
   "metadata": {},
   "source": [
    "ANSWER-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# User input for the product to search for\n",
    "search_query = input(\"Enter the product you want to search for: \")\n",
    "\n",
    "# Initialize Selenium WebDriver (you need to provide the path to your Chrome WebDriver)\n",
    "chrome_driver_path = 'path/to/your/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Create empty lists to store product details\n",
    "brands = []\n",
    "product_names = []\n",
    "prices = []\n",
    "return_exchange = []\n",
    "expected_delivery = []\n",
    "availability = []\n",
    "product_urls = []\n",
    "\n",
    "# Iterate through the first 3 pages of search results\n",
    "for page_number in range(1, 4):\n",
    "    # Construct the URL for the search results page\n",
    "    url = f\"https://www.amazon.in/s?k={search_query}&page={page_number}\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the results page to load (you may need to adjust the sleep duration)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Parse the search results page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find and scrape product details\n",
    "    product_results = soup.find_all('div', class_='s-result-item')\n",
    "    for product in product_results:\n",
    "        brand = product.find('span', class_='a-size-base-plus').text.strip()\n",
    "        product_name = product.find('span', class_='a-text-normal').text.strip()\n",
    "        price = product.find('span', class_='a-price-whole').text.strip()\n",
    "        return_exchange_info = product.find('div', class_='a-row a-size-base a-color-secondary').text.strip()\n",
    "        expected_delivery_info = product.find('div', class_='a-row a-size-base a-color-secondary').find_next_sibling().text.strip()\n",
    "        availability_info = product.find('span', class_='a-size-small a-color-secondary').text.strip()\n",
    "        product_url = product.find('a', class_='a-link-normal')['href']\n",
    "        \n",
    "        # Append details to lists\n",
    "        brands.append(brand)\n",
    "        product_names.append(product_name)\n",
    "        prices.append(price)\n",
    "        return_exchange.append(return_exchange_info)\n",
    "        expected_delivery.append(expected_delivery_info)\n",
    "        availability.append(availability_info)\n",
    "        product_urls.append(f\"https://www.amazon.in{product_url}\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    \"Brand Name\": brands,\n",
    "    \"Name of the Product\": product_names,\n",
    "    \"Price\": prices,\n",
    "    \"Return/Exchange\": return_exchange,\n",
    "    \"Expected Delivery\": expected_delivery,\n",
    "    \"Availability\": availability,\n",
    "    \"Product URL\": product_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"amazon_product_details.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'amazon_product_details.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743bf9d",
   "metadata": {},
   "source": [
    "ANSWER-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Initialize Selenium WebDriver (you need to provide the path to your Chrome WebDriver)\n",
    "chrome_driver_path = 'path/to/your/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Define a list of search keywords\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "# Create a dictionary to store image URLs for each keyword\n",
    "image_urls = {}\n",
    "\n",
    "# Access Google Images and scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    # Open Google Images\n",
    "    driver.get(\"https://www.google.com/imghp\")\n",
    "    \n",
    "    # Find the search bar and enter the keyword\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    \n",
    "    # Scroll down to load more images (you may adjust the number of scrolls)\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Parse the search results page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find and extract image URLs (up to 10 images)\n",
    "    img_tags = soup.find_all('img', class_='rg_i')\n",
    "    image_urls[keyword] = [img['src'] for img in img_tags[:10]]\n",
    "    \n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Download and save the images\n",
    "for keyword, urls in image_urls.items():\n",
    "    print(f\"Downloading {len(urls)} images for '{keyword}'\")\n",
    "    for i, url in enumerate(urls):\n",
    "        response = requests.get(url)\n",
    "        with open(f\"{keyword}_{i + 1}.jpg\", \"wb\") as img_file:\n",
    "            img_file.write(response.content)\n",
    "    print(f\"{len(urls)} images for '{keyword}' saved.\")\n",
    "\n",
    "print(\"All images downloaded and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be6bea",
   "metadata": {},
   "source": [
    "ANSWER-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# User input for the smartphone to search for\n",
    "search_query = input(\"Enter the smartphone you want to search for: \")\n",
    "\n",
    "# Construct the URL for the search results page\n",
    "url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Create empty lists to store smartphone details\n",
    "brand_names = []\n",
    "smartphone_names = []\n",
    "colors = []\n",
    "rams = []\n",
    "storages = []\n",
    "primary_cameras = []\n",
    "secondary_cameras = []\n",
    "display_sizes = []\n",
    "battery_capacities = []\n",
    "prices = []\n",
    "product_urls = []\n",
    "\n",
    "# Find and scrape product details\n",
    "product_cards = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "for product_card in product_cards:\n",
    "    brand_name = product_card.find('div', class_='_4rR01T').text.strip()\n",
    "    smartphone_name = product_card.find('a', class_='IRpwTa').text.strip()\n",
    "    color = product_card.find_all('li', class_='rgWa7D')[0].text.strip()\n",
    "    ram = product_card.find_all('li', class_='rgWa7D')[1].text.strip()\n",
    "    storage = product_card.find_all('li', class_='rgWa7D')[2].text.strip()\n",
    "    primary_camera = product_card.find_all('li', class_='rgWa7D')[3].text.strip()\n",
    "    secondary_camera = product_card.find_all('li', class_='rgWa7D')[4].text.strip()\n",
    "    display_size = product_card.find_all('li', class_='rgWa7D')[5].text.strip()\n",
    "    battery_capacity = product_card.find_all('li', class_='rgWa7D')[6].text.strip()\n",
    "    price = product_card.find('div', class_='_30jeq3').text.strip()\n",
    "    product_url = \"https://www.flipkart.com\" + product_card.find('a', class_='IRpwTa')['href']\n",
    "    \n",
    "    # Append details to lists\n",
    "    brand_names.append(brand_name)\n",
    "    smartphone_names.append(smartphone_name)\n",
    "    colors.append(color)\n",
    "    rams.append(ram)\n",
    "    storages.append(storage)\n",
    "    primary_cameras.append(primary_camera)\n",
    "    secondary_cameras.append(secondary_camera)\n",
    "    display_sizes.append(display_size)\n",
    "    battery_capacities.append(battery_capacity)\n",
    "    prices.append(price)\n",
    "    product_urls.append(product_url)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "data = {\n",
    "    \"Brand Name\": brand_names,\n",
    "    \"Smartphone Name\": smartphone_names,\n",
    "    \"Colour\": colors,\n",
    "    \"RAM\": rams,\n",
    "    \"Storage(ROM)\": storages,\n",
    "    \"Primary Camera\": primary_cameras,\n",
    "    \"Secondary Camera\": secondary_cameras,\n",
    "    \"Display Size\": display_sizes,\n",
    "    \"Battery Capacity\": battery_capacities,\n",
    "    \"Price\": prices,\n",
    "    \"Product URL\": product_urls,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"flipkart_smartphone_details.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'flipkart_smartphone_details.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e2060",
   "metadata": {},
   "source": [
    "ANSWER-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a25196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# User input for the city to search for\n",
    "city_name = input(\"Enter the city name to search for on Google Maps: \")\n",
    "\n",
    "# Initialize Selenium WebDriver (you need to provide the path to your Chrome WebDriver)\n",
    "chrome_driver_path = 'path/to/your/chromedriver.exe'\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Open Google Maps\n",
    "driver.get(\"https://maps.google.com\")\n",
    "\n",
    "# Find the search bar and enter the city name\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "search_box.clear()\n",
    "search_box.send_keys(city_name)\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the map to load (you may need to adjust the sleep duration)\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Extract the geospatial coordinates from the URL\n",
    "current_url = driver.current_url\n",
    "coords_start = current_url.index(\"@\") + 1\n",
    "coords_end = current_url.index(\",\", coords_start)\n",
    "latitude = current_url[coords_start:coords_end]\n",
    "longitude = current_url[coords_end + 1:]\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print the geospatial coordinates\n",
    "print(f\"Coordinates for {city_name}:\")\n",
    "print(f\"Latitude: {latitude}\")\n",
    "print(f\"Longitude: {longitude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2a175",
   "metadata": {},
   "source": [
    "ANSWER-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66696fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the page to scrape\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the container that holds the list of gaming laptops\n",
    "    laptops_container = soup.find(\"div\", class_=\"Top10-Seller\")\n",
    "\n",
    "    # Find and loop through individual laptop entries\n",
    "    laptop_entries = laptops_container.find_all(\"div\", class_=\"right-container\")\n",
    "    \n",
    "    for laptop_entry in laptop_entries:\n",
    "        # Extract details for each laptop\n",
    "        laptop_name = laptop_entry.find(\"div\", class_=\"TopNum1\").text.strip()\n",
    "        laptop_specs = laptop_entry.find_all(\"div\", class_=\"full-specs\")[0].text.strip()\n",
    "        laptop_price = laptop_entry.find(\"div\", class_=\"Block-price\").text.strip()\n",
    "\n",
    "        # Print the details\n",
    "        print(\"Laptop Name:\", laptop_name)\n",
    "        print(\"Specifications:\", laptop_specs)\n",
    "        print(\"Price:\", laptop_price)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa762ffb",
   "metadata": {},
   "source": [
    "ANSWER-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3478c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the Forbes billionaire list page\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table that contains the billionaire data\n",
    "    billionaire_table = soup.find(\"table\", class_=\"forbes-bill-list\")\n",
    "\n",
    "    # Find and loop through the rows of the table (skip the header row)\n",
    "    rows = billionaire_table.find_all(\"tr\")[1:]\n",
    "\n",
    "    # Create an empty list to store billionaire details\n",
    "    billionaires_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        # Extract details for each billionaire\n",
    "        columns = row.find_all(\"td\")\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        net_worth = columns[2].text.strip()\n",
    "        age = columns[3].text.strip()\n",
    "        citizenship = columns[4].text.strip()\n",
    "        source = columns[5].text.strip()\n",
    "        industry = columns[6].text.strip()\n",
    "\n",
    "        # Append details to the list\n",
    "        billionaires_data.append([rank, name, net_worth, age, citizenship, source, industry])\n",
    "\n",
    "    # Print the details for each billionaire\n",
    "    for data in billionaires_data:\n",
    "        print(\"Rank:\", data[0])\n",
    "        print(\"Name:\", data[1])\n",
    "        print(\"Net Worth:\", data[2])\n",
    "        print(\"Age:\", data[3])\n",
    "        print(\"Citizenship:\", data[4])\n",
    "        print(\"Source:\", data[5])\n",
    "        print(\"Industry:\", data[6])\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2fd1c",
   "metadata": {},
   "source": [
    "ANSWER-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0922f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "# Set your API credentials file path\n",
    "API_CREDENTIALS_FILE = 'path/to/your/credentials.json'\n",
    "\n",
    "# Set the YouTube video ID for the video you want to extract comments from\n",
    "VIDEO_ID = 'VIDEO_ID_HERE'\n",
    "\n",
    "# Initialize the YouTube Data API client\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(\n",
    "    API_CREDENTIALS_FILE, scopes)\n",
    "credentials = flow.run_console()\n",
    "youtube = googleapiclient.discovery.build(api_service_name, api_version, credentials=credentials)\n",
    "\n",
    "# Define the request to retrieve comments\n",
    "request = youtube.commentThreads().list(\n",
    "    part=\"snippet\",\n",
    "    videoId=VIDEO_ID,\n",
    "    maxResults=500,  # Set the maximum number of comments to retrieve\n",
    "    textFormat=\"plainText\"\n",
    ")\n",
    "\n",
    "# Initialize lists to store comments, comment upvotes, and comment timestamps\n",
    "comments = []\n",
    "comment_upvotes = []\n",
    "comment_timestamps = []\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "\n",
    "    while response:\n",
    "        for item in response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            upvotes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "            timestamp = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            \n",
    "            comments.append(comment)\n",
    "            comment_upvotes.append(upvotes)\n",
    "            comment_timestamps.append(timestamp)\n",
    "\n",
    "        # If there are more comments, retrieve the next page of results\n",
    "        if \"nextPageToken\" in response:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=VIDEO_ID,\n",
    "                pageToken=response[\"nextPageToken\"],\n",
    "                maxResults=500,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "except googleapiclient.errors.HttpError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Print the extracted comments, comment upvotes, and timestamps\n",
    "for i in range(len(comments)):\n",
    "    print(f\"Comment {i + 1}:\")\n",
    "    print(f\"Comment: {comments[i]}\")\n",
    "    print(f\"Upvotes: {comment_upvotes[i]}\")\n",
    "    print(f\"Timestamp: {comment_timestamps[i]}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Save the comments, upvotes, and timestamps to a file if needed\n",
    "with open('comments.txt', 'w', encoding='utf-8') as file:\n",
    "    for i in range(len(comments)):\n",
    "        file.write(f\"Comment {i + 1}:\\n\")\n",
    "        file.write(f\"Comment: {comments[i]}\\n\")\n",
    "        file.write(f\"Upvotes: {comment_upvotes[i]}\\n\")\n",
    "        file.write(f\"Timestamp: {comment_timestamps[i]}\\n\")\n",
    "        file.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "print(f\"Total comments extracted: {len(comments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5784efe",
   "metadata": {},
   "source": [
    "ANSWER-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for the search results page\n",
    "url = \"https://www.hostelworld.com/s?q=London,%20England\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the containers that hold the hostel details\n",
    "    hostel_containers = soup.find_all(\"div\", class_=\"property-card\")\n",
    "\n",
    "    # Initialize lists to store hostel details\n",
    "    hostel_names = []\n",
    "    distances = []\n",
    "    ratings = []\n",
    "    total_reviews = []\n",
    "    overall_reviews = []\n",
    "    privates_prices = []\n",
    "    dorms_prices = []\n",
    "    facilities = []\n",
    "    descriptions = []\n",
    "\n",
    "    # Loop through the hostel containers and extract details\n",
    "    for container in hostel_containers:\n",
    "        # Hostel name\n",
    "        name = container.find(\"h2\", class_=\"title\").text.strip()\n",
    "        hostel_names.append(name)\n",
    "\n",
    "        # Distance from city center\n",
    "        distance = container.find(\"span\", class_=\"distance-description\").text.strip()\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Ratings\n",
    "        rating = container.find(\"div\", class_=\"score\").text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # Total reviews\n",
    "        total_review = container.find(\"div\", class_=\"reviews\").text.strip()\n",
    "        total_reviews.append(total_review)\n",
    "\n",
    "        # Overall reviews\n",
    "        overall_review = container.find(\"div\", class_=\"keyword\").text.strip()\n",
    "        overall_reviews.append(overall_review)\n",
    "\n",
    "        # Privates from price\n",
    "        private_price = container.find(\"div\", class_=\"price-col private-price\").text.strip()\n",
    "        privates_prices.append(private_price)\n",
    "\n",
    "        # Dorms from price\n",
    "        dorm_price = container.find(\"div\", class_=\"price-col dorm-price\").text.strip()\n",
    "        dorms_prices.append(dorm_price)\n",
    "\n",
    "        # Facilities\n",
    "        facility = container.find(\"div\", class_=\"facilities\").text.strip()\n",
    "        facilities.append(facility)\n",
    "\n",
    "        # Property description\n",
    "        description = container.find(\"div\", class_=\"more-details\").text.strip()\n",
    "        descriptions.append(description)\n",
    "\n",
    "    # Print or save the scraped data\n",
    "    for i in range(len(hostel_names)):\n",
    "        print(\"Hostel Name:\", hostel_names[i])\n",
    "        print(\"Distance from City Center:\", distances[i])\n",
    "        print(\"Ratings:\", ratings[i])\n",
    "        print(\"Total Reviews:\", total_reviews[i])\n",
    "        print(\"Overall Reviews:\", overall_reviews[i])\n",
    "        print(\"Privates from Price:\", privates_prices[i])\n",
    "        print(\"Dorms from Price:\", dorms_prices[i])\n",
    "        print(\"Facilities:\", facilities[i])\n",
    "        print(\"Property Description:\", descriptions[i])\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the page. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb81a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
