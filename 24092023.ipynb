{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757ef5eb",
   "metadata": {},
   "source": [
    "Answer-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8cb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Fill in the search form and submit\n",
    "    search_url = 'https://www.shine.com/job-search/data-analyst-jobs-in-bangalore'\n",
    "    # (You may need to inspect the website's HTML to find the exact form field names)\n",
    "    payload = {\n",
    "        'search-type': 'js-auto',\n",
    "        'search-jt': 'data analyst',\n",
    "        'search-loc': 'bangalore'\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=payload)\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "\n",
    "        # Step 4: Scrape data for the first 10 jobs\n",
    "        job_listings = search_soup.find_all('div', class_='w-100 flex flex-wrap joblistborder')\n",
    "        data_list = []\n",
    "\n",
    "        for job in job_listings[:10]:\n",
    "            job_title = job.find('h2', class_='job_title').text.strip()\n",
    "            company_name = job.find('a', class_='job_comp-name').text.strip()\n",
    "            job_location = job.find('span', class_='job_loc').text.strip()\n",
    "            experience_required = job.find('span', class_='job_exp').text.strip()\n",
    "\n",
    "            job_data = {\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Job Location': job_location,\n",
    "                'Experience Required': experience_required\n",
    "            }\n",
    "            data_list.append(job_data)\n",
    "\n",
    "        # Step 5: Create a DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "else:\n",
    "    print(\"Failed to access Shine.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d77d9",
   "metadata": {},
   "source": [
    "Answer-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af1634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Fill in the search form and submit\n",
    "    search_url = 'https://www.shine.com/job-search/data-scientist-jobs-in-bangalore'\n",
    "    # (You may need to inspect the website's HTML to find the exact form field names)\n",
    "    payload = {\n",
    "        'search-type': 'js-auto',\n",
    "        'search-jt': 'data scientist',\n",
    "        'search-loc': 'bangalore'\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=payload)\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "\n",
    "        # Step 4: Scrape data for the first 10 jobs\n",
    "        job_listings = search_soup.find_all('div', class_='w-100 flex flex-wrap joblistborder')\n",
    "        data_list = []\n",
    "\n",
    "        for job in job_listings[:10]:\n",
    "            job_title = job.find('h2', class_='job_title').text.strip()\n",
    "            company_name = job.find('a', class_='job_comp-name').text.strip()\n",
    "            job_location = job.find('span', class_='job_loc').text.strip()\n",
    "\n",
    "            job_data = {\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Job Location': job_location\n",
    "            }\n",
    "            data_list.append(job_data)\n",
    "\n",
    "        # Step 5: Create a DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "else:\n",
    "    print(\"Failed to access Shine.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce1210",
   "metadata": {},
   "source": [
    "Answer-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf4f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.shine.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Fill in the search form and submit\n",
    "    search_url = 'https://www.shine.com/job-search/data-scientist-jobs'\n",
    "    # (You may need to inspect the website's HTML to find the exact form field names)\n",
    "    payload = {\n",
    "        'search-type': 'js-auto',\n",
    "        'search-jt': 'data scientist'\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=payload)\n",
    "\n",
    "    if search_response.status_code == 200:\n",
    "        search_soup = BeautifulSoup(search_response.text, 'html.parser')\n",
    "\n",
    "        # Step 3: Apply location and salary filters\n",
    "        # You will need to inspect the HTML to find the specific elements to interact with\n",
    "        # For example, the location and salary checkboxes\n",
    "\n",
    "        # Step 4: Scrape data for the first 10 jobs\n",
    "        job_listings = search_soup.find_all('div', class_='w-100 flex flex-wrap joblistborder')\n",
    "        data_list = []\n",
    "\n",
    "        for job in job_listings[:10]:\n",
    "            job_title = job.find('h2', class_='job_title').text.strip()\n",
    "            company_name = job.find('a', class_='job_comp-name').text.strip()\n",
    "            job_location = job.find('span', class_='job_loc').text.strip()\n",
    "            experience_required = job.find('span', class_='job_exp').text.strip()\n",
    "\n",
    "            job_data = {\n",
    "                'Job Title': job_title,\n",
    "                'Company Name': company_name,\n",
    "                'Job Location': job_location,\n",
    "                'Experience Required': experience_required\n",
    "            }\n",
    "            data_list.append(job_data)\n",
    "\n",
    "        # Step 6: Create a DataFrame\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Display the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to fetch search results.\")\n",
    "else:\n",
    "    print(\"Failed to access Shine.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25562f8e",
   "metadata": {},
   "source": [
    "Answer-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc35bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e14865c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m product_data\n\u001b[1;32m---> 32\u001b[0m sunglasses_data \u001b[38;5;241m=\u001b[39m scrape_sunglasses_data()\n",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m, in \u001b[0;36mscrape_sunglasses_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m listings \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_1AtVbE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listing \u001b[38;5;129;01min\u001b[39;00m listings:\n\u001b[1;32m---> 14\u001b[0m     brand \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2WkVRV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m     description \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     price \u001b[38;5;241m=\u001b[39m listing\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def scrape_sunglasses_data():\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = base_url + \"/search?q=sunglasses\"\n",
    "\n",
    "    product_data = []\n",
    "\n",
    "    while len(product_data) < 100:\n",
    "        response = requests.get(search_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "        \n",
    "        for listing in listings:\n",
    "            brand = listing.find(\"div\", class_=\"_2WkVRV\").text.strip()\n",
    "            description = listing.find(\"a\", class_=\"IRpwTa\").text.strip()\n",
    "            price = listing.find(\"div\", class_=\"_30jeq3\").text.strip()\n",
    "\n",
    "            product_data.append({\n",
    "                \"Brand\": brand,\n",
    "                \"ProductDescription\": description,\n",
    "                \"Price\": price\n",
    "            })\n",
    "\n",
    "        next_button = soup.find(\"a\", class_=\"ge-49M\")\n",
    "        if next_button:\n",
    "            search_url = base_url + next_button[\"href\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return product_data\n",
    "\n",
    "sunglasses_data = scrape_sunglasses_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023cb64",
   "metadata": {},
   "source": [
    "Answer-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "397b58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3b0fb80",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m review_data\n\u001b[1;32m---> 32\u001b[0m iphone11_reviews \u001b[38;5;241m=\u001b[39m scrape_iphone11_reviews()\n",
      "Cell \u001b[1;32mIn[27], line 14\u001b[0m, in \u001b[0;36mscrape_iphone11_reviews\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m reviews \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_27M-vq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m reviews:\n\u001b[1;32m---> 14\u001b[0m     rating \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhGSR34\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     15\u001b[0m     summary \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_2-N8zT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     16\u001b[0m     full_review \u001b[38;5;241m=\u001b[39m review\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwjRop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def scrape_iphone11_reviews():\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    product_url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "    review_data = []\n",
    "\n",
    "    while len(review_data) < 100:\n",
    "        response = requests.get(product_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        reviews = soup.find_all(\"div\", class_=\"_27M-vq\")\n",
    "        \n",
    "        for review in reviews:\n",
    "            rating = review.find(\"div\", class_=\"hGSR34\").text.strip()\n",
    "            summary = review.find(\"p\", class_=\"_2-N8zT\").text.strip()\n",
    "            full_review = review.find(\"div\", class_=\"qwjRop\").text.strip()\n",
    "\n",
    "            review_data.append({\n",
    "                \"Rating\": rating,\n",
    "                \"ReviewSummary\": summary,\n",
    "                \"FullReview\": full_review\n",
    "            })\n",
    "\n",
    "        next_button = soup.find(\"a\", class_=\"ge-49M\")\n",
    "        if next_button:\n",
    "            product_url = base_url + next_button[\"href\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return review_data\n",
    "\n",
    "iphone11_reviews = scrape_iphone11_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d909fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf5a9744",
   "metadata": {},
   "source": [
    "Answer-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5afeaeeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m product_listings \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_1AtVbE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m product_listings[:\u001b[38;5;241m100\u001b[39m]:  \u001b[38;5;66;03m# Limit to the first 100 sneakers\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Extract Brand, Product Description, and Price\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     brand \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_2WkVRV\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     26\u001b[0m     description \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     27\u001b[0m     price \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_30jeq3\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "# Set the URL for searching sneakers\n",
    "url = 'https://www.flipkart.com/search?q=sneakers'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the product listings on the page\n",
    "    product_listings = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "    for product in product_listings[:100]:  # Limit to the first 100 sneakers\n",
    "        # Extract Brand, Product Description, and Price\n",
    "        brand = product.find('div', class_='_2WkVRV').text\n",
    "        description = product.find('a', class_='IRpwTa').text\n",
    "        price = product.find('div', class_='_30jeq3').text\n",
    "\n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    data = {\n",
    "        'Brand': brands,\n",
    "        'Product Description': descriptions,\n",
    "        'Price': prices\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to access Flipkart.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5796d",
   "metadata": {},
   "source": [
    "Answer-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a907551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title             Ratings  \\\n",
      "0                                                N/A                 N/A   \n",
      "1                                                N/A                 N/A   \n",
      "2  Acer Aspire Lite 11th Gen Intel Core i3 Premiu...  3.9 out of 5 stars   \n",
      "3  HP Laptop 15s, Intel Celeron N4500, 15.6 inch(...  3.3 out of 5 stars   \n",
      "4  HP Laptop 15s, 12th Gen Intel Core i3-1215U, 1...  4.1 out of 5 stars   \n",
      "5                                                N/A                 N/A   \n",
      "6  ASUS [SmartChoice] Vivobook 15, Intel Celeron ...  3.9 out of 5 stars   \n",
      "7  Dell 14 Laptop, Intel Core 11th Gen i3-1115G4/...  4.0 out of 5 stars   \n",
      "8  HP Laptop 15s, 11th Gen Intel Core i5-1155G7, ...  4.1 out of 5 stars   \n",
      "9  Acer Aspire Lite 11th Gen Intel Core i5-1155G7...  3.5 out of 5 stars   \n",
      "\n",
      "     Price  \n",
      "0      N/A  \n",
      "1      N/A  \n",
      "2  ₹31,990  \n",
      "3  ₹28,990  \n",
      "4  ₹40,927  \n",
      "5      N/A  \n",
      "6  ₹30,990  \n",
      "7  ₹37,490  \n",
      "8  ₹54,990  \n",
      "9  ₹41,999  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "# Set the URL for searching laptops\n",
    "url = 'https://www.amazon.in/s?k=Laptop'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the filter for \"Intel Core i7\" and apply it\n",
    "    filter_element = soup.find('a', {'href': '/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031'})\n",
    "    if filter_element:\n",
    "        filter_url = 'https://www.amazon.in' + filter_element['href']\n",
    "        response = requests.get(filter_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        else:\n",
    "            print(\"Failed to apply CPU Type filter.\")\n",
    "            exit()\n",
    "\n",
    "    # Find all the laptop listings on the page\n",
    "    laptop_listings = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "    for laptop in laptop_listings[:10]:  # Limit to the first 10 laptops\n",
    "        # Extract Title, Ratings, and Price\n",
    "        title_element = laptop.find('span', class_='a-text-normal')\n",
    "        rating_element = laptop.find('span', class_='a-declarative')\n",
    "        price_element = laptop.find('span', class_='a-offscreen')\n",
    "\n",
    "        title = title_element.text.strip() if title_element else 'N/A'\n",
    "        rating = rating_element.text.strip() if rating_element else 'N/A'\n",
    "        price = price_element.text.strip() if price_element else 'N/A'\n",
    "\n",
    "        titles.append(title)\n",
    "        ratings.append(rating)\n",
    "        prices.append(price)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    data = {\n",
    "        'Title': titles,\n",
    "        'Ratings': ratings,\n",
    "        'Price': prices\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to access Amazon.in\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48f490",
   "metadata": {},
   "source": [
    "Answer-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5804c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to 'Top Quotes' page not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store data\n",
    "quotes_list = []\n",
    "authors_list = []\n",
    "types_list = []\n",
    "\n",
    "# Set the URL for the top quotes page\n",
    "url = 'https://www.azquotes.com/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the link to the \"Top Quotes\" page and click it\n",
    "    top_quotes_link = soup.find('a', {'href': '/top-quotes'})\n",
    "    if top_quotes_link:\n",
    "        top_quotes_url = 'https://www.azquotes.com' + top_quotes_link['href']\n",
    "        response = requests.get(top_quotes_url)\n",
    "\n",
    "        # Check if the request to the \"Top Quotes\" page was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all the quote blocks on the page\n",
    "            quote_blocks = soup.find_all('div', class_='wrap-block')\n",
    "\n",
    "            for block in quote_blocks:\n",
    "                # Extract Quote, Author, and Type of Quote\n",
    "                quote = block.find('a', {'title': 'view quote'}).text.strip()\n",
    "                author = block.find('a', {'title': 'view author'}).text.strip()\n",
    "                type_of_quote = block.find('a', {'title': 'view type'}).text.strip()\n",
    "\n",
    "                quotes_list.append(quote)\n",
    "                authors_list.append(author)\n",
    "                types_list.append(type_of_quote)\n",
    "\n",
    "            # Create a DataFrame from the collected data\n",
    "            data = {\n",
    "                'Quote': quotes_list,\n",
    "                'Author': authors_list,\n",
    "                'Type Of Quote': types_list\n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            # Display the DataFrame\n",
    "            print(df.head(1000))  # Print the top 1000 quotes\n",
    "\n",
    "        else:\n",
    "            print(\"Failed to access the 'Top Quotes' page.\")\n",
    "    else:\n",
    "        print(\"Link to 'Top Quotes' page not found.\")\n",
    "\n",
    "else:\n",
    "    print(\"Failed to access AzQuotes.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395dc68",
   "metadata": {},
   "source": [
    "Answer-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "627fc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\AppData\\Local\\Temp\\ipykernel_11296\\1165085152.py:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  gk_link = soup.find('a', text='GK')\n",
      "C:\\Users\\ss\\AppData\\Local\\Temp\\ipykernel_11296\\1165085152.py:26: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  pm_link = soup.find('a', text='List of all Prime Ministers of India')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.jagranjosh.com/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Find the \"GK\" option and click on it (you will need to inspect the website's HTML)\n",
    "    gk_link = soup.find('a', text='GK')\n",
    "    if gk_link:\n",
    "        gk_url = gk_link['href']\n",
    "        response = requests.get(gk_url)\n",
    "\n",
    "        # Check if the request to the \"GK\" page was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Step 3: Find the link for \"List of all Prime Ministers of India\" and click on it\n",
    "            pm_link = soup.find('a', text='List of all Prime Ministers of India')\n",
    "            if pm_link:\n",
    "                pm_url = pm_link['href']\n",
    "                response = requests.get(pm_url)\n",
    "\n",
    "                # Check if the request to the \"List of Prime Ministers\" page was successful\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Step 4: Scrape the data for respected former Prime Ministers and create a DataFrame\n",
    "                    # (You will need to inspect the HTML to locate the data you want to scrape)\n",
    "                    \n",
    "                    # Example:\n",
    "                    # prime_ministers_data = []\n",
    "                    # for row in soup.find_all('tr'):\n",
    "                    #     columns = row.find_all('td')\n",
    "                    #     if len(columns) == 4:\n",
    "                    #         name = columns[0].text.strip()\n",
    "                    #         born_dead = columns[1].text.strip()\n",
    "                    #         term_of_office = columns[2].text.strip()\n",
    "                    #         remarks = columns[3].text.strip()\n",
    "                    #         prime_ministers_data.append([name, born_dead, term_of_office, remarks])\n",
    "                    \n",
    "                    # df = pd.DataFrame(prime_ministers_data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])\n",
    "                    \n",
    "                    # Print or manipulate the DataFrame as needed\n",
    "                    # print(df)\n",
    "\n",
    "                else:\n",
    "                    print(\"Failed to access the 'List of Prime Ministers' page.\")\n",
    "            else:\n",
    "                print(\"Link for 'List of all Prime Ministers of India' not found on the 'GK' page.\")\n",
    "        else:\n",
    "            print(\"Failed to access the 'GK' page.\")\n",
    "    else:\n",
    "        print(\"Link for 'GK' not found on the homepage.\")\n",
    "else:\n",
    "    print(\"Failed to access Jagran Josh website.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa28e52",
   "metadata": {},
   "source": [
    "Answer-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1fda4cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m search_form \u001b[38;5;241m=\u001b[39m search_bar\u001b[38;5;241m.\u001b[39mfind_parent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Submit the search form\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(search_form[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m], data\u001b[38;5;241m=\u001b[39msearch_form\u001b[38;5;241m.\u001b[39mserialize())\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Check if the search request was successful\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = 'https://www.motor1.com/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Step 2: Find the search bar and type '50 most expensive cars'\n",
    "    search_bar = soup.find('input', {'name': 'q'})\n",
    "    if search_bar:\n",
    "        search_bar['value'] = '50 most expensive cars'\n",
    "        search_form = search_bar.find_parent('form')\n",
    "\n",
    "        # Submit the search form\n",
    "        response = requests.post(search_form['action'], data=search_form.serialize())\n",
    "\n",
    "        # Check if the search request was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Step 3: Find the link for '50 most expensive cars in the world' and click on it\n",
    "            link = soup.find('a', {'title': '50 Most Expensive Cars In The World'})\n",
    "            if link:\n",
    "                expensive_cars_url = link['href']\n",
    "                response = requests.get(expensive_cars_url)\n",
    "\n",
    "                # Check if the request to the '50 most expensive cars' page was successful\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Step 4: Scrape the data for the 50 most expensive cars and create a DataFrame\n",
    "                    car_data = []\n",
    "\n",
    "                    car_listings = soup.find_all('div', class_='slide slide-m1-expensive-cars')\n",
    "                    for car in car_listings:\n",
    "                        car_name = car.find('h2').text.strip()\n",
    "                        car_price = car.find('div', class_='price').text.strip()\n",
    "\n",
    "                        car_data.append([car_name, car_price])\n",
    "\n",
    "                    df = pd.DataFrame(car_data, columns=['Car Name', 'Price'])\n",
    "\n",
    "                    # Display the DataFrame\n",
    "                    print(df)\n",
    "\n",
    "                else:\n",
    "                    print(\"Failed to access the '50 Most Expensive Cars' page.\")\n",
    "            else:\n",
    "                print(\"Link for '50 Most Expensive Cars In The World' not found.\")\n",
    "        else:\n",
    "            print(\"Failed to submit the search form.\")\n",
    "    else:\n",
    "        print(\"Search bar not found on the homepage.\")\n",
    "else:\n",
    "    print(\"Failed to access Motor1.com.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779739af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
