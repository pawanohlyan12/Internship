{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30621fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1 Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a517a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_wikipedia_headers(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all header tags (h1, h2, h3, h4, h5, h6)\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # Extract the text content of the header tags\n",
    "    headers = [tag.text.strip() for tag in header_tags]\n",
    "\n",
    "    # Create a DataFrame from the header data\n",
    "    df = pd.DataFrame({'Header Tags': headers})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"  # Replace with the Wikipedia URL you want to scrape\n",
    "header_df = scrape_wikipedia_headers(url)\n",
    "print(header_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0f674af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q 2. Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://currentaffairs.adda247.com/list-of-presidents-of-india/ and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d37efafc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://currentaffairs.adda247.com/list-of-presidents-of-india/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Scrape the data and create a DataFrame\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m presidents_df \u001b[38;5;241m=\u001b[39m scrape_indian_presidents(url)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(presidents_df)\n",
      "Cell \u001b[1;32mIn[18], line 20\u001b[0m, in \u001b[0;36mscrape_indian_presidents\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     17\u001b[0m terms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Loop through rows of the table to extract data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# Start from the second row to skip headers\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_indian_presidents(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the list of former presidents\n",
    "    table = soup.find('table', class_='tablepress tablepress-id-313')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:]:  # Start from the second row to skip headers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 2:\n",
    "            name = columns[0].text.strip()\n",
    "            term = columns[1].text.strip()\n",
    "            names.append(name)\n",
    "            terms.append(term)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL of the page containing the list of former presidents of India\n",
    "url = \"https://currentaffairs.adda247.com/list-of-presidents-of-india/\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "# Display the DataFrame\n",
    "print(presidents_df)\n",
    "presidents_df = scrape_indian_presidents(url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(presidents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a59b0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7270bba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      27  3,112    115\n",
      "1     Pakistan\\nPAK      27  3,102    115\n",
      "2        India\\nIND      40  4,558    114\n",
      "3      England\\nENG      28  2,942    105\n",
      "4  South Africa\\nSA      23  2,386    104\n",
      "5   New Zealand\\nNZ      31  3,110    100\n",
      "6   Bangladesh\\nBAN      33  3,107     94\n",
      "7     Sri Lanka\\nSL      37  3,448     93\n",
      "8  Afghanistan\\nAFG      21  1,687     80\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_odi_teams(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI teams\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 ODI teams in men's cricket\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_odi_teams_df = scrape_top_10_odi_teams(odi_teams_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_odi_teams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "399af26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0378424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    863\n",
      "1           Shubman Gill  IND    759\n",
      "2  Rassie van der Dussen   SA    745\n",
      "3           David Warner  AUS    739\n",
      "4            Imam-ul-Haq  PAK    735\n",
      "5           Harry Tector  IRE    726\n",
      "6        Quinton de Kock   SA    721\n",
      "7            Virat Kohli  IND    715\n",
      "8           Rohit Sharma  IND    707\n",
      "9           Fakhar Zaman  PAK    705\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_odi_batsmen(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI batsmen\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    batsmen = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 batsmen\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            batsman = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            batsmen.append(batsman)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Batsman': batsmen, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 ODI batsmen\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_odi_batsmen_df = scrape_top_10_odi_batsmen(odi_batsmen_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_odi_batsmen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0513ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98348db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Bowler Team Rating\n",
      "0    Josh Hazlewood  AUS    692\n",
      "1    Mitchell Starc  AUS    666\n",
      "2       Trent Boult   NZ    666\n",
      "3        Adam Zampa  AUS    663\n",
      "4        Matt Henry   NZ    658\n",
      "5  Mujeeb Ur Rahman  AFG    657\n",
      "6     Kuldeep Yadav  IND    656\n",
      "7       Rashid Khan  AFG    655\n",
      "8    Mohammed Siraj  IND    643\n",
      "9    Shaheen Afridi  PAK    635\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_odi_bowlers(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI bowlers\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    bowlers = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 bowlers\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            bowler = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            bowlers.append(bowler)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Bowler': bowlers, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 ODI bowlers\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_odi_bowlers_df = scrape_top_10_odi_bowlers(odi_bowlers_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_odi_bowlers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "750733e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27177a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      26  4,290    165\n",
      "1      England\\nENG      31  3,875    125\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      30  3,039    101\n",
      "4   New Zealand\\nNZ      28  2,688     96\n",
      "5   West Indies\\nWI      29  2,743     95\n",
      "6   Bangladesh\\nBAN      17  1,284     76\n",
      "7     Sri Lanka\\nSL      12    820     68\n",
      "8     Thailand\\nTHA      13    883     68\n",
      "9     Pakistan\\nPAK      27  1,678     62\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_odi_womens_teams(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI women's teams\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 5:\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 ODI women's teams\n",
    "odi_womens_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_odi_womens_teams_df = scrape_top_10_odi_womens_teams(odi_womens_teams_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_odi_womens_teams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d52aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e024bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    801\n",
      "1           Beth Mooney  AUS    751\n",
      "2   Chamari Athapaththu   SL    743\n",
      "3       Laura Wolvaardt   SA    708\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    702\n",
      "6      Harmanpreet Kaur  IND    694\n",
      "7          Ellyse Perry  AUS    686\n",
      "8           Meg Lanning  AUS    682\n",
      "9       Stafanie Taylor   WI    618\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_womens_odi_batsmen(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 women's ODI batting players\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    batsmen = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 batsmen\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            batsman = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            batsmen.append(batsman)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Batsman': batsmen, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 women's ODI batting players\n",
    "womens_odi_batsmen_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_womens_odi_batsmen_df = scrape_top_10_womens_odi_batsmen(womens_odi_batsmen_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_womens_odi_batsmen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40242bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4436a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            All-Rounder Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    398\n",
      "1      Ashleigh Gardner  AUS    389\n",
      "2       Hayley Matthews   WI    382\n",
      "3        Marizanne Kapp   SA    362\n",
      "4          Ellyse Perry  AUS    329\n",
      "5           Amelia Kerr   NZ    328\n",
      "6         Deepti Sharma  IND    312\n",
      "7         Jess Jonassen  AUS    241\n",
      "8         Sophie Devine   NZ    233\n",
      "9              Nida Dar  PAK    217\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_top_10_womens_odi_allrounders(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 women's ODI all-rounders\n",
    "    table = soup.find('table', class_='table')\n",
    "\n",
    "    # Initialize lists to store the data\n",
    "    allrounders = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through rows of the table to extract data\n",
    "    for row in table.find_all('tr')[1:11]:  # Start from the second row to skip headers and limit to top 10 all-rounders\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 3:\n",
    "            allrounder = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            allrounders.append(allrounder)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'All-Rounder': allrounders, 'Team': teams, 'Rating': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the top 10 women's ODI all-rounders\n",
    "womens_odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "# Scrape the data and create a DataFrame\n",
    "top_10_womens_odi_allrounders_df = scrape_top_10_womens_odi_allrounders(womens_odi_allrounders_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(top_10_womens_odi_allrounders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc73fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "# Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a85f1807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news_headlines(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the news headlines\n",
    "    headlines = soup.find_all('h3', class_='Card-title')\n",
    "\n",
    "    # Initialize a list to store the headlines\n",
    "    news_headlines = []\n",
    "\n",
    "    # Loop through the headlines and extract the text\n",
    "    for headline in headlines:\n",
    "        news_headlines.append(headline.text.strip())\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Headline': news_headlines})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for CNBC world news\n",
    "cnbc_world_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Scrape the news headlines and create a DataFrame\n",
    "news_headlines_df = scrape_cnbc_news_headlines(cnbc_world_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(news_headlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e38f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "# Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "285205c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news_times(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the news publication times\n",
    "    time_elements = soup.find_all('time', class_='Card-timestamp')\n",
    "\n",
    "    # Initialize a list to store the publication times\n",
    "    news_times = []\n",
    "\n",
    "    # Loop through the time elements and extract the text\n",
    "    for time_element in time_elements:\n",
    "        news_times.append(time_element.text.strip())\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Time': news_times})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for CNBC world news\n",
    "cnbc_world_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Scrape the news publication times and create a DataFrame\n",
    "news_times_df = scrape_cnbc_news_times(cnbc_world_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(news_times_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38f8f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "# News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9f7f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news_links(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the news links\n",
    "    link_elements = soup.find_all('a', class_='Card-headline')\n",
    "\n",
    "    # Initialize a list to store the news links\n",
    "    news_links = []\n",
    "\n",
    "    # Loop through the link elements and extract the href attribute\n",
    "    for link_element in link_elements:\n",
    "        news_link = link_element.get('href')\n",
    "        if news_link:\n",
    "            news_links.append(news_link)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'News Link': news_links})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for CNBC world news\n",
    "cnbc_world_url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Scrape the news links and create a DataFrame\n",
    "news_links_df = scrape_cnbc_news_links(cnbc_world_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(news_links_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d51c1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "# Paper Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4f1728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_ai_articles(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the paper titles\n",
    "    title_elements = soup.find_all('h2', class_='issue-item-title')\n",
    "\n",
    "    # Initialize a list to store the paper titles\n",
    "    paper_titles = []\n",
    "\n",
    "    # Loop through the title elements and extract the text\n",
    "    for title_element in title_elements:\n",
    "        paper_title = title_element.text.strip()\n",
    "        paper_titles.append(paper_title)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Paper Title': paper_titles})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the most downloaded articles in AI in the last 90 days\n",
    "ai_articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Scrape the paper titles and create a DataFrame\n",
    "ai_articles_df = scrape_most_downloaded_ai_articles(ai_articles_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(ai_articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf78438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "# Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84959f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Authors]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_ai_articles(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the author names\n",
    "    author_elements = soup.find_all('span', class_='textInLine')\n",
    "\n",
    "    # Initialize a list to store the author names\n",
    "    authors = []\n",
    "\n",
    "    # Loop through the author elements and extract the text\n",
    "    for author_element in author_elements:\n",
    "        author_name = author_element.text.strip()\n",
    "        authors.append(author_name)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Authors': authors})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the most downloaded articles in AI in the last 90 days\n",
    "ai_articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Scrape the authors' names and create a DataFrame\n",
    "ai_authors_df = scrape_most_downloaded_ai_articles(ai_articles_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(ai_authors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ba3ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "# Published Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c79ffe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Published Date]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_ai_articles(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the published dates\n",
    "    date_elements = soup.find_all('div', class_='text-xs')\n",
    "\n",
    "    # Initialize a list to store the published dates\n",
    "    published_dates = []\n",
    "\n",
    "    # Loop through the date elements and extract the text\n",
    "    for date_element in date_elements:\n",
    "        published_date = date_element.text.strip()\n",
    "        published_dates.append(published_date)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Published Date': published_dates})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the most downloaded articles in AI in the last 90 days\n",
    "ai_articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Scrape the published dates and create a DataFrame\n",
    "ai_published_dates_df = scrape_most_downloaded_ai_articles(ai_articles_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(ai_published_dates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da4df619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "# Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3c1776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_ai_articles(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the paper URLs\n",
    "    paper_elements = soup.find_all('a', class_='anchor', href=True)\n",
    "\n",
    "    # Initialize a list to store the paper URLs\n",
    "    paper_urls = []\n",
    "\n",
    "    # Loop through the paper elements and extract the href attribute\n",
    "    for paper_element in paper_elements:\n",
    "        paper_url = paper_element.get('href')\n",
    "        if paper_url:\n",
    "            paper_urls.append(paper_url)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Paper URL': paper_urls})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for the most downloaded articles in AI in the last 90 days\n",
    "ai_articles_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "# Scrape the paper URLs and create a DataFrame\n",
    "ai_paper_urls_df = scrape_most_downloaded_ai_articles(ai_articles_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(ai_paper_urls_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b43f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# Restaurant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14834b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_restaurant_names(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the restaurant names\n",
    "    name_elements = soup.find_all('div', class_='restnt-info-detail')\n",
    "\n",
    "    # Initialize a list to store the restaurant names\n",
    "    restaurant_names = []\n",
    "\n",
    "    # Loop through the name elements and extract the text\n",
    "    for name_element in name_elements:\n",
    "        restaurant_name = name_element.find('span', class_='restnt-name').text.strip()\n",
    "        restaurant_names.append(restaurant_name)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Restaurant Name': restaurant_names})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for dineout.co.in\n",
    "dineout_url = \"https://www.dineout.co.in/\"\n",
    "\n",
    "# Scrape the restaurant names and create a DataFrame\n",
    "restaurant_names_df = scrape_dineout_restaurant_names(dineout_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_names_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9716a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "867c1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Cuisine]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_cuisines(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the restaurant cuisines\n",
    "    cuisine_elements = soup.find_all('div', class_='restnt-info-detail')\n",
    "\n",
    "    # Initialize a list to store the restaurant cuisines\n",
    "    cuisines = []\n",
    "\n",
    "    # Loop through the cuisine elements and extract the text\n",
    "    for cuisine_element in cuisine_elements:\n",
    "        cuisine = cuisine_element.find('span', class_='double-line-ellipsis').text.strip()\n",
    "        cuisines.append(cuisine)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Cuisine': cuisines})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for dineout.co.in\n",
    "dineout_url = \"https://www.dineout.co.in/\"\n",
    "\n",
    "# Scrape the restaurant cuisines and create a DataFrame\n",
    "restaurant_cuisines_df = scrape_dineout_cuisines(dineout_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_cuisines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e5bf8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f039e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Location]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_locations(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the restaurant locations\n",
    "    location_elements = soup.find_all('div', class_='restnt-info-detail')\n",
    "\n",
    "    # Initialize a list to store the restaurant locations\n",
    "    locations = []\n",
    "\n",
    "    # Loop through the location elements and extract the text\n",
    "    for location_element in location_elements:\n",
    "        location = location_element.find('span', class_='double-line-ellipsis').next_sibling.strip()\n",
    "        locations.append(location)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Location': locations})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for dineout.co.in\n",
    "dineout_url = \"https://www.dineout.co.in/\"\n",
    "\n",
    "# Scrape the restaurant locations and create a DataFrame\n",
    "restaurant_locations_df = scrape_dineout_locations(dineout_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_locations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1760b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1af521e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Ratings]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_ratings(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the restaurant ratings\n",
    "    rating_elements = soup.find_all('div', class_='restnt-rating')\n",
    "\n",
    "    # Initialize a list to store the restaurant ratings\n",
    "    ratings = []\n",
    "\n",
    "    # Loop through the rating elements and extract the text\n",
    "    for rating_element in rating_elements:\n",
    "        rating = rating_element.text.strip()\n",
    "        ratings.append(rating)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Ratings': ratings})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for dineout.co.in\n",
    "dineout_url = \"https://www.dineout.co.in/\"\n",
    "\n",
    "# Scrape the restaurant ratings and create a DataFrame\n",
    "restaurant_ratings_df = scrape_dineout_ratings(dineout_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_ratings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7991c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "# Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a7eb99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_dineout_image_urls(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the elements containing the restaurant image URLs\n",
    "    image_elements = soup.find_all('div', class_='img-container')\n",
    "\n",
    "    # Initialize a list to store the restaurant image URLs\n",
    "    image_urls = []\n",
    "\n",
    "    # Loop through the image elements and extract the image URLs\n",
    "    for image_element in image_elements:\n",
    "        image_url = image_element.find('img')['src']\n",
    "        image_urls.append(image_url)\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame({'Image URL': image_urls})\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL for dineout.co.in\n",
    "dineout_url = \"https://www.dineout.co.in/\"\n",
    "\n",
    "# Scrape the restaurant image URLs and create a DataFrame\n",
    "restaurant_image_urls_df = scrape_dineout_image_urls(dineout_url)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(restaurant_image_urls_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
